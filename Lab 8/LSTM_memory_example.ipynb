{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpcMapxx6H1Y"
   },
   "source": [
    "## Memory for Recurrent Neural Networks\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlDtdbQS6EXV",
    "outputId": "cee99716-4981-4ab6-ac83-4aaae64e9223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# onehotencoding\r\n",
    "def encode(sequence, n_unique):\r\n",
    "\tencoded = list()\r\n",
    "\tfor value in sequence:\r\n",
    "\t\trow = [0.0 for x in range(n_unique)]\r\n",
    "\t\trow[value] = 1.0\r\n",
    "\t\tencoded.append(row)\r\n",
    "\treturn encoded\r\n",
    " \r\n",
    "seq1 = [0, 1, 2, 3, 4, 5]\r\n",
    "encoded = encode(seq1, 6)\r\n",
    "for v in encoded:\r\n",
    "\tprint(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VQmlTj1I4oQm"
   },
   "outputs": [],
   "source": [
    "# create pairs of encoded vectors, returns X, y\r\n",
    "\r\n",
    "def train(encoded):\r\n",
    "\tX,y = list(),list()\r\n",
    "\tfor i in range(2, len(encoded)):\r\n",
    "\t\tX.append(encoded[i-2])  # input \r\n",
    "\t\ty.append(encoded[i])    # output\r\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMsG8gwG5wCh",
    "outputId": "24305b16-fdab-4091-adc1-38a98a8f2e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "seq = [3, 2, 1, 0, 4, 5, 17, 10, 20, 30, 23, 24, 6, 26, 30, 12, 14, 30, 22, 29]\r\n",
    "encoded = encode(seq, np.array(seq).max()+1)\r\n",
    "X, y = train(encoded)\r\n",
    "for i in range(len(X)):\r\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V0C80_PB6CjT"
   },
   "outputs": [],
   "source": [
    "# preprocess sequences to use in LSTM\r\n",
    "\r\n",
    "def preprocessing(sequence, n_unique):\r\n",
    "\t# onehotencoding\r\n",
    "\tencoded = encode(sequence, n_unique)\r\n",
    "\t# construct train set\r\n",
    "\tX,y = train(encoded)\r\n",
    "\t# convert and reshape for use in LSTM\r\n",
    "\tX, y = pd.DataFrame(X), pd.DataFrame(y)\r\n",
    "\tX = X.values\r\n",
    "\tX_new = X.reshape(X.shape[0], 1, X.shape[1])\r\n",
    "\tY_new = y.values\r\n",
    "\treturn X_new, Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8qB1UhGgRoIa"
   },
   "outputs": [],
   "source": [
    "sequence_X, sequence_Y = preprocessing(seq, np.array(seq).max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PyMOW39bRsnZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import LSTM\r\n",
    "\r\n",
    "# define LSTM configuration\r\n",
    "n_neurons = 64\r\n",
    "n_batch = 1\r\n",
    "n_epoch = 250\r\n",
    "n_features = np.array(seq).max()+1\r\n",
    "\r\n",
    "# construct a model\r\n",
    "model = Sequential()\r\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(1, 1, 31), stateful=True))\r\n",
    "model.add(Dense(n_features, activation='sigmoid'))\r\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8Eoian9R16o",
    "outputId": "5b6a0334-65f3-4ad7-ab1b-eecd1b9fff1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 - 1s - loss: 0.6871\n",
      "18/18 - 0s - loss: 0.6503\n",
      "18/18 - 0s - loss: 0.4203\n",
      "18/18 - 0s - loss: 0.1659\n",
      "18/18 - 0s - loss: 0.1343\n",
      "18/18 - 0s - loss: 0.1278\n",
      "18/18 - 0s - loss: 0.1250\n",
      "18/18 - 0s - loss: 0.1235\n",
      "18/18 - 0s - loss: 0.1224\n",
      "18/18 - 0s - loss: 0.1216\n",
      "18/18 - 0s - loss: 0.1210\n",
      "18/18 - 0s - loss: 0.1205\n",
      "18/18 - 0s - loss: 0.1201\n",
      "18/18 - 0s - loss: 0.1197\n",
      "18/18 - 0s - loss: 0.1193\n",
      "18/18 - 0s - loss: 0.1189\n",
      "18/18 - 0s - loss: 0.1186\n",
      "18/18 - 0s - loss: 0.1183\n",
      "18/18 - 0s - loss: 0.1179\n",
      "18/18 - 0s - loss: 0.1176\n",
      "18/18 - 0s - loss: 0.1173\n",
      "18/18 - 0s - loss: 0.1169\n",
      "18/18 - 0s - loss: 0.1166\n",
      "18/18 - 0s - loss: 0.1162\n",
      "18/18 - 0s - loss: 0.1158\n",
      "18/18 - 0s - loss: 0.1154\n",
      "18/18 - 0s - loss: 0.1150\n",
      "18/18 - 0s - loss: 0.1146\n",
      "18/18 - 0s - loss: 0.1141\n",
      "18/18 - 0s - loss: 0.1136\n",
      "18/18 - 0s - loss: 0.1131\n",
      "18/18 - 0s - loss: 0.1126\n",
      "18/18 - 0s - loss: 0.1121\n",
      "18/18 - 0s - loss: 0.1116\n",
      "18/18 - 0s - loss: 0.1110\n",
      "18/18 - 0s - loss: 0.1104\n",
      "18/18 - 0s - loss: 0.1098\n",
      "18/18 - 0s - loss: 0.1092\n",
      "18/18 - 0s - loss: 0.1086\n",
      "18/18 - 0s - loss: 0.1079\n",
      "18/18 - 0s - loss: 0.1072\n",
      "18/18 - 0s - loss: 0.1065\n",
      "18/18 - 0s - loss: 0.1058\n",
      "18/18 - 0s - loss: 0.1051\n",
      "18/18 - 0s - loss: 0.1043\n",
      "18/18 - 0s - loss: 0.1036\n",
      "18/18 - 0s - loss: 0.1028\n",
      "18/18 - 0s - loss: 0.1020\n",
      "18/18 - 0s - loss: 0.1012\n",
      "18/18 - 0s - loss: 0.1004\n",
      "18/18 - 0s - loss: 0.0996\n",
      "18/18 - 0s - loss: 0.0987\n",
      "18/18 - 0s - loss: 0.0978\n",
      "18/18 - 0s - loss: 0.0970\n",
      "18/18 - 0s - loss: 0.0961\n",
      "18/18 - 0s - loss: 0.0952\n",
      "18/18 - 0s - loss: 0.0943\n",
      "18/18 - 0s - loss: 0.0933\n",
      "18/18 - 0s - loss: 0.0924\n",
      "18/18 - 0s - loss: 0.0914\n",
      "18/18 - 0s - loss: 0.0904\n",
      "18/18 - 0s - loss: 0.0894\n",
      "18/18 - 0s - loss: 0.0883\n",
      "18/18 - 0s - loss: 0.0873\n",
      "18/18 - 0s - loss: 0.0862\n",
      "18/18 - 0s - loss: 0.0851\n",
      "18/18 - 0s - loss: 0.0839\n",
      "18/18 - 0s - loss: 0.0827\n",
      "18/18 - 0s - loss: 0.0816\n",
      "18/18 - 0s - loss: 0.0804\n",
      "18/18 - 0s - loss: 0.0792\n",
      "18/18 - 0s - loss: 0.0780\n",
      "18/18 - 0s - loss: 0.0768\n",
      "18/18 - 0s - loss: 0.0755\n",
      "18/18 - 0s - loss: 0.0743\n",
      "18/18 - 0s - loss: 0.0731\n",
      "18/18 - 0s - loss: 0.0719\n",
      "18/18 - 0s - loss: 0.0707\n",
      "18/18 - 0s - loss: 0.0696\n",
      "18/18 - 0s - loss: 0.0684\n",
      "18/18 - 0s - loss: 0.0672\n",
      "18/18 - 0s - loss: 0.0661\n",
      "18/18 - 0s - loss: 0.0650\n",
      "18/18 - 0s - loss: 0.0638\n",
      "18/18 - 0s - loss: 0.0627\n",
      "18/18 - 0s - loss: 0.0616\n",
      "18/18 - 0s - loss: 0.0605\n",
      "18/18 - 0s - loss: 0.0595\n",
      "18/18 - 0s - loss: 0.0584\n",
      "18/18 - 0s - loss: 0.0574\n",
      "18/18 - 0s - loss: 0.0563\n",
      "18/18 - 0s - loss: 0.0553\n",
      "18/18 - 0s - loss: 0.0542\n",
      "18/18 - 0s - loss: 0.0532\n",
      "18/18 - 0s - loss: 0.0522\n",
      "18/18 - 0s - loss: 0.0511\n",
      "18/18 - 0s - loss: 0.0501\n",
      "18/18 - 0s - loss: 0.0491\n",
      "18/18 - 0s - loss: 0.0481\n",
      "18/18 - 0s - loss: 0.0472\n",
      "18/18 - 0s - loss: 0.0462\n",
      "18/18 - 0s - loss: 0.0453\n",
      "18/18 - 0s - loss: 0.0444\n",
      "18/18 - 0s - loss: 0.0435\n",
      "18/18 - 0s - loss: 0.0426\n",
      "18/18 - 0s - loss: 0.0417\n",
      "18/18 - 0s - loss: 0.0409\n",
      "18/18 - 0s - loss: 0.0401\n",
      "18/18 - 0s - loss: 0.0393\n",
      "18/18 - 0s - loss: 0.0385\n",
      "18/18 - 0s - loss: 0.0377\n",
      "18/18 - 0s - loss: 0.0370\n",
      "18/18 - 0s - loss: 0.0363\n",
      "18/18 - 0s - loss: 0.0356\n",
      "18/18 - 0s - loss: 0.0349\n",
      "18/18 - 0s - loss: 0.0342\n",
      "18/18 - 0s - loss: 0.0336\n",
      "18/18 - 0s - loss: 0.0329\n",
      "18/18 - 0s - loss: 0.0323\n",
      "18/18 - 0s - loss: 0.0317\n",
      "18/18 - 0s - loss: 0.0311\n",
      "18/18 - 0s - loss: 0.0305\n",
      "18/18 - 0s - loss: 0.0299\n",
      "18/18 - 0s - loss: 0.0294\n",
      "18/18 - 0s - loss: 0.0288\n",
      "18/18 - 0s - loss: 0.0283\n",
      "18/18 - 0s - loss: 0.0277\n",
      "18/18 - 0s - loss: 0.0272\n",
      "18/18 - 0s - loss: 0.0267\n",
      "18/18 - 0s - loss: 0.0262\n",
      "18/18 - 0s - loss: 0.0258\n",
      "18/18 - 0s - loss: 0.0253\n",
      "18/18 - 0s - loss: 0.0248\n",
      "18/18 - 0s - loss: 0.0244\n",
      "18/18 - 0s - loss: 0.0240\n",
      "18/18 - 0s - loss: 0.0235\n",
      "18/18 - 0s - loss: 0.0231\n",
      "18/18 - 0s - loss: 0.0227\n",
      "18/18 - 0s - loss: 0.0223\n",
      "18/18 - 0s - loss: 0.0219\n",
      "18/18 - 0s - loss: 0.0215\n",
      "18/18 - 0s - loss: 0.0212\n",
      "18/18 - 0s - loss: 0.0208\n",
      "18/18 - 0s - loss: 0.0204\n",
      "18/18 - 0s - loss: 0.0201\n",
      "18/18 - 0s - loss: 0.0197\n",
      "18/18 - 0s - loss: 0.0194\n",
      "18/18 - 0s - loss: 0.0191\n",
      "18/18 - 0s - loss: 0.0188\n",
      "18/18 - 0s - loss: 0.0185\n",
      "18/18 - 0s - loss: 0.0181\n",
      "18/18 - 0s - loss: 0.0178\n",
      "18/18 - 0s - loss: 0.0176\n",
      "18/18 - 0s - loss: 0.0173\n",
      "18/18 - 0s - loss: 0.0170\n",
      "18/18 - 0s - loss: 0.0167\n",
      "18/18 - 0s - loss: 0.0164\n",
      "18/18 - 0s - loss: 0.0162\n",
      "18/18 - 0s - loss: 0.0159\n",
      "18/18 - 0s - loss: 0.0157\n",
      "18/18 - 0s - loss: 0.0154\n",
      "18/18 - 0s - loss: 0.0152\n",
      "18/18 - 0s - loss: 0.0149\n",
      "18/18 - 0s - loss: 0.0147\n",
      "18/18 - 0s - loss: 0.0145\n",
      "18/18 - 0s - loss: 0.0142\n",
      "18/18 - 0s - loss: 0.0140\n",
      "18/18 - 0s - loss: 0.0138\n",
      "18/18 - 0s - loss: 0.0136\n",
      "18/18 - 0s - loss: 0.0134\n",
      "18/18 - 0s - loss: 0.0132\n",
      "18/18 - 0s - loss: 0.0130\n",
      "18/18 - 0s - loss: 0.0128\n",
      "18/18 - 0s - loss: 0.0126\n",
      "18/18 - 0s - loss: 0.0124\n",
      "18/18 - 0s - loss: 0.0122\n",
      "18/18 - 0s - loss: 0.0120\n",
      "18/18 - 0s - loss: 0.0118\n",
      "18/18 - 0s - loss: 0.0116\n",
      "18/18 - 0s - loss: 0.0115\n",
      "18/18 - 0s - loss: 0.0113\n",
      "18/18 - 0s - loss: 0.0111\n",
      "18/18 - 0s - loss: 0.0110\n",
      "18/18 - 0s - loss: 0.0108\n",
      "18/18 - 0s - loss: 0.0106\n",
      "18/18 - 0s - loss: 0.0105\n",
      "18/18 - 0s - loss: 0.0103\n",
      "18/18 - 0s - loss: 0.0102\n",
      "18/18 - 0s - loss: 0.0100\n",
      "18/18 - 0s - loss: 0.0099\n",
      "18/18 - 0s - loss: 0.0097\n",
      "18/18 - 0s - loss: 0.0096\n",
      "18/18 - 0s - loss: 0.0095\n",
      "18/18 - 0s - loss: 0.0093\n",
      "18/18 - 0s - loss: 0.0092\n",
      "18/18 - 0s - loss: 0.0091\n",
      "18/18 - 0s - loss: 0.0089\n",
      "18/18 - 0s - loss: 0.0088\n",
      "18/18 - 0s - loss: 0.0087\n",
      "18/18 - 0s - loss: 0.0086\n",
      "18/18 - 0s - loss: 0.0084\n",
      "18/18 - 0s - loss: 0.0083\n",
      "18/18 - 0s - loss: 0.0082\n",
      "18/18 - 0s - loss: 0.0081\n",
      "18/18 - 0s - loss: 0.0080\n",
      "18/18 - 0s - loss: 0.0079\n",
      "18/18 - 0s - loss: 0.0078\n",
      "18/18 - 0s - loss: 0.0077\n",
      "18/18 - 0s - loss: 0.0075\n",
      "18/18 - 0s - loss: 0.0074\n",
      "18/18 - 0s - loss: 0.0073\n",
      "18/18 - 0s - loss: 0.0072\n",
      "18/18 - 0s - loss: 0.0071\n",
      "18/18 - 0s - loss: 0.0070\n",
      "18/18 - 0s - loss: 0.0069\n",
      "18/18 - 0s - loss: 0.0069\n",
      "18/18 - 0s - loss: 0.0068\n",
      "18/18 - 0s - loss: 0.0067\n",
      "18/18 - 0s - loss: 0.0066\n",
      "18/18 - 0s - loss: 0.0065\n",
      "18/18 - 0s - loss: 0.0064\n",
      "18/18 - 0s - loss: 0.0063\n",
      "18/18 - 0s - loss: 0.0062\n",
      "18/18 - 0s - loss: 0.0062\n",
      "18/18 - 0s - loss: 0.0061\n",
      "18/18 - 0s - loss: 0.0060\n",
      "18/18 - 0s - loss: 0.0059\n",
      "18/18 - 0s - loss: 0.0058\n",
      "18/18 - 0s - loss: 0.0058\n",
      "18/18 - 0s - loss: 0.0057\n",
      "18/18 - 0s - loss: 0.0056\n",
      "18/18 - 0s - loss: 0.0055\n",
      "18/18 - 0s - loss: 0.0055\n",
      "18/18 - 0s - loss: 0.0054\n",
      "18/18 - 0s - loss: 0.0053\n",
      "18/18 - 0s - loss: 0.0053\n",
      "18/18 - 0s - loss: 0.0052\n",
      "18/18 - 0s - loss: 0.0051\n",
      "18/18 - 0s - loss: 0.0051\n",
      "18/18 - 0s - loss: 0.0050\n",
      "18/18 - 0s - loss: 0.0049\n",
      "18/18 - 0s - loss: 0.0049\n",
      "18/18 - 0s - loss: 0.0048\n",
      "18/18 - 0s - loss: 0.0047\n",
      "18/18 - 0s - loss: 0.0047\n",
      "18/18 - 0s - loss: 0.0046\n",
      "18/18 - 0s - loss: 0.0046\n",
      "18/18 - 0s - loss: 0.0045\n",
      "18/18 - 0s - loss: 0.0045\n",
      "18/18 - 0s - loss: 0.0044\n"
     ]
    }
   ],
   "source": [
    "# train LSTM\r\n",
    "for i in range(n_epoch):\r\n",
    "\tmodel.fit(sequence_X, sequence_Y, epochs=1, batch_size=1, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fs4lNIO6R7yE",
    "outputId": "8b9f26bc-152d-43a7-f63e-70d5b6b33ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence\n",
      "[3, 2, 1, 0, 4, 5, 17, 10, 20, 30, 23, 24, 6, 26, 30, 12, 14, 30, 22, 29]\n",
      "X = 3.0, prediction = 1.0\n",
      "X = 2.0, prediction = 0.0\n",
      "X = 1.0, prediction = 4.0\n",
      "X = 0.0, prediction = 5.0\n",
      "X = 4.0, prediction = 17.0\n",
      "X = 5.0, prediction = 10.0\n",
      "X = 17.0, prediction = 20.0\n",
      "X = 10.0, prediction = 30.0\n",
      "X = 20.0, prediction = 23.0\n",
      "X = 30.0, prediction = 24.0\n",
      "X = 23.0, prediction = 6.0\n",
      "X = 24.0, prediction = 26.0\n",
      "X = 6.0, prediction = 30.0\n",
      "X = 26.0, prediction = 12.0\n",
      "X = 30.0, prediction = 14.0\n",
      "X = 12.0, prediction = 30.0\n",
      "X = 14.0, prediction = 22.0\n",
      "X = 30.0, prediction = 29.0\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict(sequence_X, batch_size=n_batch), axis=-1) \r\n",
    "\r\n",
    "# test LSTM on sequence 2\r\n",
    "print('Sequence')\r\n",
    "print(seq)\r\n",
    "for i in range(len(result)):\r\n",
    "\tprint('X = %.1f, prediction = %.1f' % (seq[i], result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I45JT2cO7yEz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_memory_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
