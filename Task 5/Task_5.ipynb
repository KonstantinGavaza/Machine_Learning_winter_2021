{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decreased-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = codecs.open('Text.txt', 'r', 'utf_8_sig').read()\n",
    "data_chapter = []\n",
    "cur_index = 0\n",
    "i = 0\n",
    "while data.find('CHAPTER', cur_index) != -1:\n",
    "    i += 1\n",
    "    with codecs.open('Text' + str(i)  + '.txt', 'w', 'utf_8_sig') as file:\n",
    "        file.write(data[data.find('CHAPTER', cur_index): data.find('CHAPTER', data.find('CHAPTER', cur_index) + 1)])\n",
    "    cur_index = data.find('CHAPTER', cur_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "august-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1: ['wa', 'little', 'way', 'like', 'think', 'get', 'see', 'door', 'well', 'thought']\n",
      "Chapter 2: ['wa', 'little', 'mouse', 'said', 'dear', 'pool', 'went', 'cat', 'foot', 'like']\n",
      "Chapter 3: ['said', 'mouse', 'wa', 'know', 'dodo', 'lory', 'dry', 'one', 'prize', 'could']\n",
      "Chapter 4: ['wa', 'little', 'said', 'rabbit', 'one', 'window', 'bill', 'heard', 'thought', 'sure']\n",
      "Chapter 5: ['said', 'caterpillar', 'wa', 'serpent', 'pigeon', 'see', 'well', 'little', 'know', 'think']\n",
      "Chapter 6: ['said', 'wa', 'cat', 'like', 'know', 'duchess', 'mad', 'little', 'much', 'footman']\n",
      "Chapter 7: ['said', 'hatter', 'march', 'hare', 'dormouse', 'wa', 'know', 'time', 'well', 'thing']\n",
      "Chapter 8: ['said', 'queen', 'wa', 'head', 'king', 'like', 'cat', 'three', 'went', 'one']\n",
      "Chapter 9: ['said', 'turtle', 'mock', 'wa', 'gryphon', 'duchess', 'went', 'queen', 'little', 'never']\n",
      "Chapter 10: ['said', 'gryphon', 'turtle', 'mock', 'lobster', 'beautiful', 'soup', 'soooop', 'would', 'whiting']\n",
      "Chapter 11: ['said', 'wa', 'king', 'hatter', 'court', 'dormouse', 'one', 'witness', 'queen', 'thought']\n",
      "Chapter 12: ['said', 'king', 'would', 'wa', 'queen', 'know', 'jury', 'little', 'one', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "verbs = []\n",
    "def get_verbs(tag):\n",
    "    if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return True\n",
    "    return False\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    words = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    for word in words:\n",
    "        if get_verbs(word[1]) and len(word[0])>2:\n",
    "            verbs.append(word[0])\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text, re.UNICODE)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = ['be' if obj in ['im']  else obj for obj in text]\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stop_words += ['oh']\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def all_text(count = 12):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    for i in range(1, count + 1):\n",
    "        data = pd.read_csv('Text' + str(i) + '.txt', delimiter=\"\\t\")\n",
    "        \n",
    "        data.iloc[:,0] = data.iloc[:,0].apply(lambda x: clean_text(x))\n",
    "        \n",
    "        X = data.iloc[:,0].tolist()\n",
    "    \n",
    "        vectorizer_tftidf = TfidfVectorizer()\n",
    "        \n",
    "        X = vectorizer_tftidf.fit_transform(X)\n",
    "        idx = np.ravel(X.sum(axis=0).argsort(axis=1))[::-1][:11]\n",
    "        top_10_words = np.array(vectorizer_tftidf.get_feature_names())[idx].tolist()\n",
    "        top_10_words.remove('alice')\n",
    "        print('Chapter {i}:'.format(i = i), top_10_words)\n",
    "\n",
    "all_text()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Text.txt', delimiter=\"\\t\")\n",
    "data.iloc[:,0] = data.iloc[:,0].apply(lambda x: clean_text(x))\n",
    "X = data.iloc[:,0].tolist()\n",
    "X = [X[i].split() for i in range(len(X))]\n",
    "verbs = set(verbs)\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "graphic-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(X, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attached-costs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14325, 390300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X, total_examples=model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "awful-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2475\n"
     ]
    }
   ],
   "source": [
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "concerned-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('said', 0.9946261644363403)\n",
      "('thought', 0.9899784326553345)\n",
      "('began', 0.9878082275390625)\n",
      "('sure', 0.98622065782547)\n",
      "('went', 0.9853482246398926)\n",
      "('know', 0.9842615723609924)\n",
      "('get', 0.9839959144592285)\n",
      "('replied', 0.9830548763275146)\n",
      "('like', 0.9820861220359802)\n",
      "('got', 0.9808074235916138)\n",
      "('came', 0.9799314737319946)\n",
      "('dear', 0.9793164730072021)\n",
      "('looked', 0.9778375625610352)\n",
      "('getting', 0.9769870042800903)\n",
      "('come', 0.9764481782913208)\n",
      "('say', 0.9763820767402649)\n",
      "('next', 0.9757989048957825)\n"
     ]
    }
   ],
   "source": [
    "ans = model.wv.most_similar(positive=[\"alice\"], topn = 40)\n",
    "for obj in ans: \n",
    "    if (obj[0] in verbs):\n",
    "        print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-bikini",
   "metadata": {},
   "source": [
    "Thus, the most common verbs are: ['say', 'thought', 'began', 'like', 'go', 'get', 'know', 'reply', 'look', 'came']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-schedule",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
